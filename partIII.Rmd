---
title: "Part III"
author: "Group 25"
date: "`r Sys.Date()`"
output: pdf_document
---


# load the library

```{r}
library(tidyverse)
library(knitr)
library(dplyr)
library(car)
```

# extract the subset of data we need

```{r}
coffee_df <- read.csv(file = "merged_data_cleaned.csv") %>% 
  rename("Ratings" = Total.Cup.Points) %>% 
  mutate(.before = 1, ISpecies = ifelse(Species == "Arabica", 1, 0)) %>% 
  select(Species, ISpecies, Aroma, Flavor, Aftertaste, Acidity, Ratings)
# write.csv(coffee_df,file = "CoffeeRatings.csv", row.names = FALSE)
```



# numerical summary

```{r}
summary(coffee_df)

summary_table <- coffee_df %>%
  summarise(
    Aroma_mean = mean(Aroma),
    Flavor_mean = mean(Flavor),
    Aftertaste_mean = mean(Aftertaste),
    Acidity_mean = mean(Acidity),
    Aroma_sd = sd(Aroma),
    Flavor_sd = sd(Flavor),
    Aftertaste_sd = sd(Aftertaste),
    Acidity_sd = sd(Acidity),
    Aroma_max = max(Aroma),
    Flavor_max = max(Flavor),
    Aftertaste_max = max(Aftertaste),
    Acidity_max = max(Acidity),
    Aroma_min = min(Aroma),
    Flavor_min = min(Flavor),
    Aftertaste_min = min(Aftertaste),
    Acidity_min = min(Acidity), 
)

summary_frame = data.frame(
  Variables = c("Flavor", "Aroma", "Aftertaste", "Acidity"),
  Min = c(summary_table$Flavor_min, summary_table$Aroma_min, summary_table$Aftertaste_min, summary_table$Acidity_min),
  Max = c(summary_table$Flavor_max, summary_table$Aroma_max, summary_table$Aftertaste_max, summary_table$Acidity_max),
  Mean = c(summary_table$Flavor_mean, summary_table$Aroma_mean, summary_table$Aftertaste_mean, summary_table$Acidity_mean),
  SD = c(summary_table$Flavor_sd, summary_table$Aroma_sd, summary_table$Aftertaste_sd, summary_table$Acidity_sd)
)

kable(summary_frame, format = "markdown", caption = "Coffee Ratings Dataset Numerical Summary", 
      col.names = c("Variable Name", "Minimum", "Maximum", "Mean", "Standard Deviation"), 
      align = "c", longtable = TRUE, digits = 3)

coffee_df %>% group_by(Species) %>% 
  summarise(num_species = n(), Proportion = round(num_species/nrow(coffee_df), 3), mean_rating= round(mean(Ratings), 3)) %>% 
  rename(`Number of Observations` = num_species, 'Mean Rating' = mean_rating) %>% 
  kable(caption = "Coffee Ratings Dataset Categorical Variable Summary", align = "c", longtable = TRUE)
```






The coffee ratings dataset contains 1339 observations, with 1311 of these being of the "Arabica" species, while the other 28 are of the "Robusta" species. The minimum for all variables is 0, while the maximum rating is given in the Sweetness category. Sweetness also has the highest mean and standard deviation.

# Fit the model

```{r, eval=FALSE}
model <- lm(Ratings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity, data = coffee_df)
summary(model)
```


# identification of problematic observations
do not remove any problematic oberservations expect the "0" row we observed in the 
dataset, others are served as limitations of our model.
```{r}
n <- nrow(coffee_df)
p <- length(coef(model)) - 1 # minus intercept
# leverage point
hii <- hatvalues(model)
hii_cutoff <- 2 * (p + 1)/n
leverage <- as.vector(which(hii > hii_cutoff))

# outlier points
ri <- rstandard(model)
outlier <- as.vector(which(ri > 4 | ri < -4))

# influential on all fitted values
di <- cooks.distance(model)
di_cutoff <- qf(0.5, p + 1, n - p - 1)
infl_fitted <- as.vector(which(di > di_cutoff))

# influential on own fitted values
dffits <- dffits(model)
dffits_cutoff <- 2 * sqrt((p + 1)/n)
infl_ofitted <- as.vector(which(abs(dffits)>dffits_cutoff))

# influential on coefficients
dfbetas <- dfbetas(model)
dfbetas_cutoff <- 2/ sqrt(dim(dfbetas)[1])
# dim(dfbetas)
infl_coef <- c()
for (i in c(1:dim(dfbetas)[2])) {
  infl_coef <- append(infl_coef, as.vector(which(abs(dfbetas) > dfbetas_cutoff)))
  # print(as.vector(which(abs(dfbetas) > dfbetas_cutoff)))
  
}

# problematic observations that appear in both criteria
i <- Reduce(intersect, list(leverage, outlier, infl_fitted, infl_ofitted, 
                            infl_coef))
i
```
observation # 1311 is problematic, and it has 0 in the rating, so we decide to 
remove this observation.




# Remove observation with rating of 0

```{r}

coffee_df2 <- coffee_df[-c(1311),]
```

# exploratoty data analysis
```{r}
par(mfrow = c(2, 3))
for (i in c(2:7)){
  hist(coffee_df2[,i], main = "", xlab = names(coffee_df2)[i])
}
```
from the histogram, we found that the majority species in this dataset is 
"Arabica" while the minority is the "Robusta". The coffee data was collected 
mainly from Arabica species, may indicate an issue with uncorrelated error.

After removing the "0" row, there is still left skewness in "Sweetness" predictor
and also a left skewness in the response "Rating". Since sever skewness presented
in both predictor and response variables, there may be an issue with linearity and
normality.



# Refit model with new data
```{r}
model2 <- lm(Ratings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity, data = coffee_df2)
summary(model2)
```


# check residual plots above with coffee_df2
```{r}
y_hat_2 <- fitted(model2)
# residual
e_hat_2 <- resid(model2)
```

# check condition
# condition 1: response vs. fitted -- conditional mean response
# condition 2: pairwise scatter plot of all predictors -- conditional mean predictors
```{r}

plot(x = y_hat_2, y = coffee_df2$Ratings, main = "Response vs. Fitted", xlab = "Fitted", 
     ylab = "Response")
abline(coef = c(0,1), lty = 2)
title(sub = ~italic("Fig.4: The response (coffee ratings) versus fitted values plot from the model."))


```
```{r}
pairs(coffee_df2[,2:7])
title(sub = ~italic("Fig.5: The pairwise scatter plots of all predictors."))
```





# Normality 
```{r}

qqnorm(e_hat_2)
qqline(e_hat_2)
title(sub = ~italic("Fig.3: The normal Q-Q plot of residuals from the model."))

```

# Residual vs fitted plot
```{r}
plot(x = y_hat_2, y = e_hat_2, main = "Residual vs Fitted", xlab = "Fitted", 
     ylab = "Residuals")
title(sub = ~italic("Fig.1: The residuals versus fitted values plot from the model."))
```

# Boxplot for species
```{r}

boxplot(e_hat_2 ~ Species, data = coffee_df2, ylab = "Residual", main = "Residual by Coffee Species")
title(sub = ~italic("Fig.2b: Distribution of Residual vs. Coffee Species"))

```

# Predictor vs residual
```{r}
par(mfrow = c(2, 3),oma=c(4, 2, 0, 0)+0.1)

for (i in c(2:6)){
  plot(x = coffee_df2[, i], y = e_hat_2, xlab = names(coffee_df2)[i],
       ylab = "Residual", main = paste("Residual vs. ", names(coffee_df2)[i]))
}

mtext(~italic("Fig.2a: All residuals versus predictors plots from the model."), side = 1, line = 3, outer = TRUE, adj = 0.5)

```

* from the exploratory data analysis, we discovered that left-skewness presented
in the response variable (coffee ratings), also presented in the predictors (sweetness)
. Additionally, the majority of data are collected from species "Arabica", indicated
a potential violation in the uncorrelated error assumption.

* Both the conditional mean response and conditional mean predictors conditions are
satisfied, so we can trust the interpretation of the residual plots when checking 
assumptions.

* assumptions:
1. the normality assumption is violated: significant off-diagonal trend presented 
in the normal qq plot.
2. the constant variance assumption is violated: since there is a decreasing fanning 
pattern in the residual vs. fitted as well the residual vs. predictors plot.
3. the uncorrelated error assumption is violated: since clusters appears in the residual
plots.
4. the linearity assumption is satisfied since no non-linear patterns appeared in 
residual plots.

Based on these evidence, to mitigate the violation
we decided to apply a squares transformation to the response variable 


```{r}
# apply Box-Cox transformation to mitigate 
# violation in normality violation

library(car)
boxCox(model2)

# try transformation on the predictors & response
p <- powerTransform(cbind(coffee_df2[,c(3:7)]))
summary(p)
# the powers are high for both predictors
# and response
# for model simplicity and interpretability of our model
#, we decide not to apply this transformation
```

```{r}
# variance stabilizing transformation
# we spotted the left-skew in response distribution 
# and a fanning pattern with decreasing spread in the residual plot





# log transformation
coffee_df2$lgRatings <- log(coffee_df2$Ratings)
model3 <- lm(lgRatings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity,data = coffee_df2)
y_hat_3 <- fitted(model3)
e_hat_3 <- resid(model3)


# squares transformation
coffee_df2$sqRatings <- (coffee_df2$Ratings)^2
model4 <- lm(sqRatings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity,data = coffee_df2)
y_hat_4 <- fitted(model4)
e_hat_4 <- resid(model4)


# cube root transformation
coffee_df2$cuberootRatings <- (coffee_df2$Ratings)^(1/3)
model5 <- lm(cuberootRatings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity,data = coffee_df2)
y_hat_5 <- fitted(model5)
e_hat_5 <- resid(model5)



# before transformation
# hist(coffee_df2[,"Ratings"], main = "Distribution of Ratings", xlab = "Ratings")
# plot(y = e_hat_2, x = y_hat_2, main = "Residual vs Fitted", xlab = "Fitted", 
#      ylab = "Residuals")
par(mfrow = c(3, 2))

hist(coffee_df2[, "lgRatings"], main = "Distribution of lgRatings", xlab = "lgRatings")
plot(x = y_hat_3, y = e_hat_3, main = "Residual vs Fitted (log)", xlab = "Fitted", 
     ylab = "Residuals")

hist(coffee_df2[, "sqRatings"], main = "Distribution of sqRatings", xlab = "sqRatings")
plot(x = y_hat_4, y = e_hat_4, main = "Residual vs Fitted (Squares)", xlab = "Fitted", 
     ylab = "Residuals")

hist(coffee_df2[, "cuberootRatings"], main = "Distribution of cuberootRatings", xlab = "cuberootRatings")
plot(x = y_hat_5, y = e_hat_5, main = "Residual vs Fitted (Cuberoot)", xlab = "Fitted", 
     ylab = "Residuals")

# none of these transformation improved either the skewness in the repsonse
# or the violation in constant variance assumption
# not to transform



```
```{r}
# qq plot after potential transformation

```


