---
title: "STA302 Project Part III"
author: "Group 25"
date: "`r Sys.Date()`"
output: pdf_document
---



```{r, echo = TRUE, include=FALSE}
library(tidyverse)
library(knitr)
library(dplyr)
library(car)
```

## extract the subset of data we need

```{r}
coffee_df <- read.csv(file = "merged_data_cleaned.csv") %>% 
  rename("Ratings" = Total.Cup.Points) %>% 
  mutate(.before = 1, ISpecies = ifelse(Species == "Arabica", 1, 0)) %>% 
  select(Species, ISpecies, Aroma, Flavor, Aftertaste, Acidity, Ratings)
# write.csv(coffee_df,file = "CoffeeRatings.csv", row.names = FALSE)
```



## numerical summary

```{r}
summary(coffee_df)

summary_table <- coffee_df %>%
  summarise(
    Aroma_mean = mean(Aroma),
    Flavor_mean = mean(Flavor),
    Aftertaste_mean = mean(Aftertaste),
    Acidity_mean = mean(Acidity),
    Aroma_sd = sd(Aroma),
    Flavor_sd = sd(Flavor),
    Aftertaste_sd = sd(Aftertaste),
    Acidity_sd = sd(Acidity),
    Aroma_max = max(Aroma),
    Flavor_max = max(Flavor),
    Aftertaste_max = max(Aftertaste),
    Acidity_max = max(Acidity),
    Aroma_min = min(Aroma),
    Flavor_min = min(Flavor),
    Aftertaste_min = min(Aftertaste),
    Acidity_min = min(Acidity), 
)

summary_frame = data.frame(
  Variables = c("Flavor", "Aroma", "Aftertaste", "Acidity"),
  Min = c(summary_table$Flavor_min, 
          summary_table$Aroma_min, summary_table$Aftertaste_min, 
          summary_table$Acidity_min),
  Max = c(summary_table$Flavor_max, summary_table$Aroma_max, 
          summary_table$Aftertaste_max, summary_table$Acidity_max),
  Mean = c(summary_table$Flavor_mean, summary_table$Aroma_mean, 
           summary_table$Aftertaste_mean, summary_table$Acidity_mean),
  SD = c(summary_table$Flavor_sd, summary_table$Aroma_sd, 
         summary_table$Aftertaste_sd, summary_table$Acidity_sd)
)

kable(summary_frame, format = "markdown", caption = "Coffee Ratings Dataset Numerical Summary", 
      col.names = c("Variable Name", "Minimum", 
                    "Maximum", "Mean", "Standard Deviation"), 
      align = "c", longtable = TRUE, digits = 3)

coffee_df %>% group_by(Species) %>% 
  summarise(num_species = n(), Proportion = round(num_species/nrow(coffee_df), 3), mean_rating= round(mean(Ratings), 3)) %>% 
  rename(`Number of Observations` = num_species, 'Mean Rating' = mean_rating) %>% 
  kable(caption = "Coffee Ratings Dataset Categorical Variable Summary", 
        align = "c", longtable = TRUE)
```



The coffee ratings dataset contains 1339 observations, with 1311 of these being of the "Arabica" species, while the other 28 are of the "Robusta" species. The minimum for all variables is 0, while the maximum rating is given in the Sweetness category. Sweetness also has the highest mean and standard deviation.

## Fit the model
## identification of problematic observations
do not remove any problematic oberservations expect the "0" row we observed in the 
dataset, others are served as limitations of our model.
```{r}
model <- lm(Ratings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity, 
            data = coffee_df)
summary(model)

n <- nrow(coffee_df)
p <- length(coef(model)) - 1 # minus intercept
# leverage point
hii <- hatvalues(model)
hii_cutoff <- 2 * (p + 1)/n
leverage <- as.vector(which(hii > hii_cutoff))

# outlier points
ri <- rstandard(model)
outlier <- as.vector(which(ri > 4 | ri < -4))

# influential on all fitted values
di <- cooks.distance(model)
di_cutoff <- qf(0.5, p + 1, n - p - 1)
infl_fitted <- as.vector(which(di > di_cutoff))

# influential on own fitted values
dffits <- dffits(model)
dffits_cutoff <- 2 * sqrt((p + 1)/n)
infl_ofitted <- as.vector(which(abs(dffits)>dffits_cutoff))

# influential on coefficients
dfbetas <- dfbetas(model)
dfbetas_cutoff <- 2/ sqrt(dim(dfbetas)[1])
# dim(dfbetas)
infl_coef <- c()
for (i in c(1:dim(dfbetas)[2])) {
  infl_coef <- append(infl_coef, 
                      as.vector(which(abs(dfbetas) > dfbetas_cutoff)))
  # print(as.vector(which(abs(dfbetas) > dfbetas_cutoff)))
  
}

# problematic observations that appear in both criteria
i <- Reduce(intersect, list(leverage, outlier, infl_fitted, infl_ofitted, 
                            infl_coef))
i
```
observation # 1311 is problematic, and it has 0 in the rating, so we decide to 
remove this observation.

Remove observation with rating of 0

```{r}

coffee_df2 <- coffee_df[-c(1311),]
```

## exploratoty data analysis
```{r}
par(mfrow = c(2, 3))
for (i in c(2:7)){
  hist(coffee_df2[,i], main = "", xlab = names(coffee_df2)[i])
}
```
from the histogram, we found that the majority species in this dataset is 
"Arabica" while the minority is the "Robusta". The coffee data was collected 
mainly from Arabica species, may indicate an issue with uncorrelated error.

After removing the "0" row, there is still left skewness in "Sweetness" predictor
and also a left skewness in the response "Rating". Since sever skewness presented
in both predictor and response variables, there may be an issue with linearity and
normality.



## Refit model with new data
```{r}
model2 <- lm(Ratings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity, data = coffee_df2)
summary(model2)
```


## check residual plots above with coffee_df2
```{r}
y_hat_2 <- fitted(model2)
# residual
e_hat_2 <- resid(model2)
```

## check condition
## condition 1: response vs. fitted -- conditional mean response
## condition 2: pairwise scatter plot of all predictors -- conditional mean predictors
```{r}

plot(x = y_hat_2, y = coffee_df2$Ratings, main = "Response vs. Fitted", xlab = "Fitted", 
     ylab = "Response")
abline(coef = c(0,1), lty = 2)
title(sub = ~italic("Fig.4: The response (coffee ratings) versus fitted values plot from the model."))


```
```{r}
pairs(coffee_df2[,2:7])
title(sub = ~italic("Fig.5: The pairwise scatter plots of all predictors."))
```





## Normality 
```{r}

qqnorm(e_hat_2)
qqline(e_hat_2)
title(sub = ~italic("Fig.3: The normal Q-Q plot of residuals from the model."))

```

## Residual vs fitted plot
```{r}
plot(x = y_hat_2, y = e_hat_2, main = "Residual vs Fitted", xlab = "Fitted", 
     ylab = "Residuals")
title(sub = ~italic("Fig.1: The residuals versus fitted values plot from the model."))
```

## Boxplot for species
```{r}

boxplot(e_hat_2 ~ Species, data = coffee_df2, ylab = "Residual", main = "Residual by Coffee Species")
title(sub = ~italic("Fig.2b: Distribution of Residual vs. Coffee Species"))

```

## Predictor vs residual
```{r}
par(mfrow = c(2, 3),oma=c(4, 2, 0, 0)+0.1)

for (i in c(2:6)){
  plot(x = coffee_df2[, i], y = e_hat_2, xlab = names(coffee_df2)[i],
       ylab = "Residual", main = paste("Residual vs. ", names(coffee_df2)[i]))
}

mtext(~italic("Fig.2a: All residuals versus predictors plots from the model."), side = 1, line = 3, outer = TRUE, adj = 0.5)

```

* from the exploratory data analysis, we discovered that left-skewness presented
in the response variable (coffee ratings), also presented in the predictors (sweetness)
. Additionally, the majority of data are collected from species "Arabica", indicated
a potential violation in the uncorrelated error assumption.

* Both the conditional mean response and conditional mean predictors conditions are
satisfied, so we can trust the interpretation of the residual plots when checking 
assumptions.

* assumptions:
1. the normality assumption is violated: significant off-diagonal trend presented 
in the normal qq plot.
2. the constant variance assumption is violated: since there is a decreasing fanning 
pattern in the residual vs. fitted as well the residual vs. predictors plot.
3. the uncorrelated error assumption is violated: since clusters appears in the residual
plots.
4. the linearity assumption is satisfied since no non-linear patterns appeared in 
residual plots.

Based on these evidence, to mitigate the violation
we decided to apply a squares transformation to the response variable 


```{r}
# apply Box-Cox transformation to mitigate 
# violation in normality violation

library(car)
boxCox(model2)

# try transformation on the predictors & response
p <- powerTransform(cbind(coffee_df2[,c(3:7)]))
summary(p)
# the powers are high for both predictors
# and response
# for model simplicity and interpretability of our model
#, we decide not to apply this transformation
```

```{r}
# Attempt simultaneous BoxCox transformation on response and predictors
coffee_df2$BoxCoxRatings <- (coffee_df2$Ratings)^11
model6 = lm(BoxCoxRatings ~ ISpecies + Aroma^4 + Flavor^4 + Aftertaste^4 + Acidity^4, data = coffee_df2)
y_hat_6 <- fitted(model6)
e_hat_6 <- resid(model6)
qqnorm(e_hat_6)
qqline(e_hat_6)
# Transformations do not improve normality violation
```


```{r}
# variance stabilizing transformation
# we spotted the left-skew in response distribution 
# and a fanning pattern with decreasing spread in the residual plot





# log transformation
coffee_df2$lgRatings <- log(coffee_df2$Ratings)
model3 <- lm(lgRatings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity,data = coffee_df2)
y_hat_3 <- fitted(model3)
e_hat_3 <- resid(model3)


# squares transformation
coffee_df2$sqRatings <- (coffee_df2$Ratings)^2
model4 <- lm(sqRatings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity,data = coffee_df2)
y_hat_4 <- fitted(model4)
e_hat_4 <- resid(model4)


# cube root transformation
coffee_df2$cuberootRatings <- (coffee_df2$Ratings)^(1/3)
model5 <- lm(cuberootRatings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity,data = coffee_df2)
y_hat_5 <- fitted(model5)
e_hat_5 <- resid(model5)



# before transformation
# hist(coffee_df2[,"Ratings"], main = "Distribution of Ratings", xlab = "Ratings")
# plot(y = e_hat_2, x = y_hat_2, main = "Residual vs Fitted", xlab = "Fitted", 
#      ylab = "Residuals")
par(mfrow = c(2, 4))

hist(coffee_df2[, "Ratings"], xlab = "Ratings", main = "")
plot(x = y_hat_2, y = e_hat_2, xlab = "Fitted", 
     ylab = "Residuals")

hist(coffee_df2[, "lgRatings"], xlab = "lgRatings", main = "")
plot(x = y_hat_3, y = e_hat_3, xlab = "Fitted", 
     ylab = "Residuals")

hist(coffee_df2[, "sqRatings"], xlab = "sqRatings", main = "")
plot(x = y_hat_4, y = e_hat_4, xlab = "Fitted", 
     ylab = "Residuals")

hist(coffee_df2[, "cuberootRatings"], xlab = "cuberootRatings", main = "")
plot(x = y_hat_5, y = e_hat_5, xlab = "Fitted", 
     ylab = "Residuals")

# none of these transformation improved either the skewness in the repsonse
# or the violation in constant variance assumption




```
```{r}
# qq plot after potential transformation
# does not have any improvement
par(mfrow = c(2, 2))
qqnorm(e_hat_2)
qqline(e_hat_2)
qqnorm(e_hat_3,main = "Normal Q-Q Plot (log)")
qqline(e_hat_3)
qqnorm(e_hat_4,main = "Normal Q-Q Plot (Squares)")
qqline(e_hat_4)
qqnorm(e_hat_5,main = "Normal Q-Q Plot (CubeRoot)")
qqline(e_hat_5)
```
After trying the possible variance-stabilizing transformation and Box-Cox
transformation, we keep the same model (model2) as it is before the transformation, 
since the transformation did not mitigate the violation and skewness in the 
response variable.
We accepted the violations in our model as a caveat of the natural of data.

## ANOVA test for overall significance
* $H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = 0$
* $H_1:$ at least one $\beta_j$ != $0$

notice that the p-value of model2 less than $2.2 \times 10^{-16}$
therefore, we reject null and conclude significant linear relationship
exists for at least one predictor.
```{r}
summary(model2)
```
From the summary table, notice that there is a significant linear relationship 
between every predictor and coffee ratings in the presence of other predictors,
we conclude that every predictor is linearly related to the coffee rating.
We want to see whether the reduced model may better than the full model.

## Partial F Test
```{r}
# full model contains ISpecies, Aroma, Flavor, Aftertaste, Acidity

# refit a model not using acidity
model6 <- lm(Ratings ~ ISpecies + Aroma + Flavor + Aftertaste, data = coffee_df2)
summary(model6)

anova(model6, model2)
# since p-value = 9.022e-08 ***, we reject the null
# do not discard

# not using Aftertaste
model7 <- lm(Ratings ~ ISpecies + Aroma + Flavor + Acidity, data = coffee_df2)
summary(model7)

anova(model7, model2)

# not using Flavor
model8 <- lm(Ratings ~ ISpecies + Aroma + Aftertaste + Acidity, data = coffee_df2)
anova(model8, model2)

# not using Aroma
model9 <- lm(Ratings ~ ISpecies + Flavor + Aftertaste + Acidity, 
             data = coffee_df2)
anova(model9, model2)

# not using ISpecies
model10 <- lm(Ratings ~ Aroma + Flavor + Aftertaste + Acidity, data = coffee_df2)
anova(model10, model2)


```

```{r}
# Recheck problematic observations, VIF, and model goodness

n <- nrow(coffee_df2)
p <- length(coef(model2)) - 1 # minus intercept

# leverage point
hii <- hatvalues(model2)
hii_cutoff <- 2 * (p + 1)/n
leverage <- as.vector(which(hii > hii_cutoff))


# outlier points
ri <- rstandard(model2)
outlier <- as.vector(which(ri > 4 | ri < -4))


# influential on all fitted values
di <- cooks.distance(model2)
di_cutoff <- qf(0.5, p + 1, n - p - 1)
infl_fitted <- as.vector(which(di > di_cutoff))


# influential on own fitted values
dffits <- dffits(model2)
dffits_cutoff <- 2 * sqrt((p + 1)/n)
infl_ofitted <- as.vector(which(abs(dffits)>dffits_cutoff))


# influential on coefficients
dfbetas <- dfbetas(model2)
dfbetas_cutoff <- 2/ sqrt(dim(dfbetas)[1])
# dim(dfbetas)
infl_coef <- c()
for (i in c(1:dim(dfbetas)[2])) {
  infl_coef <- append(infl_coef, 
                      as.vector(which(abs(dfbetas) > dfbetas_cutoff)))
  #print(as.vector(which(abs(dfbetas) > dfbetas_cutoff)))
  print(length(which(abs(dfbetas[,i]) > dfbetas_cutoff)))
}

# problematic observations that appear in both criteria
i <- Reduce(intersect, list(leverage, outlier, infl_fitted, infl_ofitted, 
                            infl_coef))
i

# Check for multicollinearity
vif(model2)

p =length(coef(model2)) - 1
n=nrow(coffee_df2)

# Calculate adjusted R^2, AIC, BIC, and AICc
cbind(summary(model2)$adj.r.squared, extractAIC(model2, k=2)[2], extractAIC(model2, k=log(n))[2],
      extractAIC(model2, k=2)[2]+(2*(p+2)*(p+3)/(n-p-1)))

```

# All possible subsets check
```{r}
# install.packages("leaps")
library(leaps)

best <- regsubsets(Ratings ~ ISpecies + Aroma + Flavor + Aftertaste + Acidity, data=coffee_df2, nbest=1, nvmax=5)
summary(best)

subsets(best, statistic="adjr2", legend=FALSE)
# Model with all five predictors has highest adjusted R squared
```
